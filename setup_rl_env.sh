#!/bin/bash

# Vision-SR1 RLè®­ç»ƒç¯å¢ƒå®‰è£…è„šæœ¬
# æ­¤è„šæœ¬ç”¨äºè®¾ç½®RLè®­ç»ƒçš„condaç¯å¢ƒ
# å‚è€ƒäº†deepeyesv2_rlç¯å¢ƒçš„é…ç½®ï¼Œä½†é€‚é…äº†Vision-SR1çš„ç‰¹å®šè¦æ±‚ï¼ˆPython 3.11ç­‰ï¼‰

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "=========================================="
echo "Vision-SR1 RLè®­ç»ƒç¯å¢ƒå®‰è£…è„šæœ¬"
echo "=========================================="
echo "æ³¨æ„: æœ¬è„šæœ¬å‚è€ƒäº†deepeyesv2_rlç¯å¢ƒçš„é…ç½®"
echo "ä½†ä½¿ç”¨Python 3.11å’ŒVision-SR1è¦æ±‚çš„ç‰¹å®šç‰ˆæœ¬"
echo ""
echo "ğŸ’¡ æç¤º: pipå’Œcondaä¼šè‡ªåŠ¨ä½¿ç”¨ç¼“å­˜"
echo "   ç›¸åŒç‰ˆæœ¬çš„åŒ…ä¼šä»ç¼“å­˜å®‰è£…ï¼ŒèŠ‚çœä¸‹è½½æ—¶é—´"
echo "   ç¯å¢ƒä¹‹é—´ä»ç„¶æ˜¯éš”ç¦»çš„ï¼Œä¸ä¼šæ··ä¹±"
echo "=========================================="

# æ£€æµ‹CUDAç‰ˆæœ¬
echo ""
echo "æ­£åœ¨æ£€æµ‹CUDAç‰ˆæœ¬..."
if command -v nvcc &> /dev/null; then
    CUDA_VERSION=$(nvcc --version | grep "release" | sed 's/.*release \([0-9]\+\.[0-9]\+\).*/\1/')
    echo "æ£€æµ‹åˆ°CUDAç‰ˆæœ¬: $CUDA_VERSION"
else
    echo "è­¦å‘Š: æœªæ£€æµ‹åˆ°nvccï¼Œå°è¯•ä»nvidia-smiè·å–CUDAç‰ˆæœ¬..."
    if command -v nvidia-smi &> /dev/null; then
        CUDA_VERSION=$(nvidia-smi | grep "CUDA Version" | sed 's/.*CUDA Version: \([0-9]\+\.[0-9]\+\).*/\1/' || echo "unknown")
        echo "æ£€æµ‹åˆ°CUDAç‰ˆæœ¬: $CUDA_VERSION"
    else
        echo "é”™è¯¯: æ— æ³•æ£€æµ‹CUDAç‰ˆæœ¬ï¼Œè¯·ç¡®ä¿å·²å®‰è£…NVIDIAé©±åŠ¨å’ŒCUDAå·¥å…·åŒ…"
        exit 1
    fi
fi

# ç¡®å®šPyTorchçš„CUDAç‰ˆæœ¬
# setup.shä¸­ä½¿ç”¨çš„æ˜¯cu124ï¼Œä½†CUDA 12.4+é€šå¸¸å¯ä»¥ä½¿ç”¨cu124çš„PyTorch
# ä½¿ç”¨ç®€å•çš„æ•°å€¼æ¯”è¾ƒï¼ˆä¸ä¾èµ–bcï¼‰
CUDA_MAJOR=$(echo "$CUDA_VERSION" | cut -d. -f1)
CUDA_MINOR=$(echo "$CUDA_VERSION" | cut -d. -f2)

if [ "$CUDA_MAJOR" -gt 12 ] || ([ "$CUDA_MAJOR" -eq 12 ] && [ "$CUDA_MINOR" -ge 4 ]); then
    # CUDA 12.4+ ä½¿ç”¨ cu124
    PYTORCH_CUDA="cu124"
    echo "å°†ä½¿ç”¨PyTorch CUDAç‰ˆæœ¬: $PYTORCH_CUDA"
elif [ "$CUDA_MAJOR" -eq 12 ] && [ "$CUDA_MINOR" -ge 1 ]; then
    # CUDA 12.1-12.3 ä½¿ç”¨ cu121
    PYTORCH_CUDA="cu121"
    echo "å°†ä½¿ç”¨PyTorch CUDAç‰ˆæœ¬: $PYTORCH_CUDA"
elif [ "$CUDA_MAJOR" -gt 11 ] || ([ "$CUDA_MAJOR" -eq 11 ] && [ "$CUDA_MINOR" -ge 8 ]); then
    # CUDA 11.8-12.0 ä½¿ç”¨ cu118
    PYTORCH_CUDA="cu118"
    echo "å°†ä½¿ç”¨PyTorch CUDAç‰ˆæœ¬: $PYTORCH_CUDA"
else
    echo "è­¦å‘Š: CUDAç‰ˆæœ¬ $CUDA_VERSION å¯èƒ½ä¸å—æ”¯æŒï¼Œå°†å°è¯•ä½¿ç”¨cu124"
    PYTORCH_CUDA="cu124"
fi

# æ£€æµ‹Pythonç‰ˆæœ¬
PYTHON_VERSION=$(python --version 2>&1 | grep -oE '[0-9]+\.[0-9]+' | head -1)
echo "Pythonç‰ˆæœ¬: $PYTHON_VERSION"

# æ£€æŸ¥æ˜¯å¦åœ¨condaç¯å¢ƒä¸­
if [ -z "$CONDA_DEFAULT_ENV" ]; then
    echo ""
    echo "è­¦å‘Š: æœªæ£€æµ‹åˆ°æ¿€æ´»çš„condaç¯å¢ƒ"
    echo "è¯·å…ˆåˆ›å»ºå¹¶æ¿€æ´»condaç¯å¢ƒ:"
    echo "  conda env create -f environment.yml"
    echo "  conda activate vision-sr1-rl"
    read -p "æ˜¯å¦ç»§ç»­? (y/n) " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        exit 1
    fi
else
    echo "å½“å‰condaç¯å¢ƒ: $CONDA_DEFAULT_ENV"
fi

echo ""
echo "å¼€å§‹å®‰è£…ä¾èµ–..."
echo ""

# å®‰è£…PyTorchï¼ˆç¬¬ä¸€æ¬¡å®‰è£…ï¼‰
echo "æ­¥éª¤ 1/8: å®‰è£…PyTorch 2.6.0..."
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/${PYTORCH_CUDA}

# å®‰è£…flash-attention
echo ""
echo "æ­¥éª¤ 2/8: å®‰è£…flash-attention 2.7.4..."
# æ ¹æ®Pythonç‰ˆæœ¬é€‰æ‹©wheelæ–‡ä»¶
PYTHON_MAJOR_MINOR=$(echo $PYTHON_VERSION | cut -d. -f1,2 | tr -d '.')
if [ "$PYTHON_MAJOR_MINOR" = "311" ]; then
    FLASH_ATTN_WHL="flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl"
elif [ "$PYTHON_MAJOR_MINOR" = "310" ]; then
    FLASH_ATTN_WHL="flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl"
elif [ "$PYTHON_MAJOR_MINOR" = "312" ]; then
    FLASH_ATTN_WHL="flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl"
else
    echo "è­¦å‘Š: Pythonç‰ˆæœ¬ $PYTHON_VERSION å¯èƒ½æ²¡æœ‰é¢„ç¼–è¯‘çš„flash-attention wheel"
    echo "å°†å°è¯•å®‰è£…é€šç”¨ç‰ˆæœ¬..."
    FLASH_ATTN_WHL="flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl"
fi

pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/${FLASH_ATTN_WHL} || {
    echo "è­¦å‘Š: flash-attentionå®‰è£…å¤±è´¥ï¼Œå¯èƒ½éœ€è¦ä»æºç ç¼–è¯‘"
    echo "å°è¯•å®‰è£…å¯ç”¨çš„ç‰ˆæœ¬..."
    pip install flash-attn>=2.4.3
}

# å®‰è£…vllmï¼ˆç¬¬ä¸€æ¬¡ï¼‰
echo ""
echo "æ­¥éª¤ 3/8: å®‰è£…vllm 0.8.4..."
pip install "vllm==0.8.4"

# é‡æ–°å®‰è£…PyTorchï¼ˆç¡®ä¿ç‰ˆæœ¬ä¸€è‡´ï¼‰
echo ""
echo "æ­¥éª¤ 4/8: é‡æ–°å®‰è£…PyTorchä»¥ç¡®ä¿ç‰ˆæœ¬ä¸€è‡´..."
pip install torch==2.6.0 --index-url https://download.pytorch.org/whl/${PYTORCH_CUDA}

# å®‰è£…transformersç‰¹å®šç‰ˆæœ¬
echo ""
echo "æ­¥éª¤ 5/8: å®‰è£…transformersç‰¹å®šç‰ˆæœ¬..."
pip install git+https://github.com/huggingface/transformers.git@1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf

# å®‰è£…é¡¹ç›®æœ¬èº«
echo ""
echo "æ­¥éª¤ 6/8: å®‰è£…Vision-SR1é¡¹ç›®..."
pip install -e .

# é‡æ–°å®‰è£…torchå’Œflash-attnï¼ˆè§£å†³å¯èƒ½çš„ä¾èµ–å†²çªï¼‰
echo ""
echo "æ­¥éª¤ 7/8: é‡æ–°å®‰è£…torchå’Œflash-attnä»¥è§£å†³ä¾èµ–å†²çª..."
pip uninstall -y torch flash-attn || true
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/${PYTORCH_CUDA}
pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/${FLASH_ATTN_WHL} || pip install flash-attn>=2.4.3

# é‡æ–°å®‰è£…vllm
echo ""
echo "æ­¥éª¤ 8/8: é‡æ–°å®‰è£…vllm..."
pip uninstall -y vllm || true
pip install "vllm==0.8.4"

# é‡æ–°å®‰è£…transformerså¹¶å›ºå®šç‰ˆæœ¬
echo ""
echo "æœ€ç»ˆæ­¥éª¤: å›ºå®štransformersç‰ˆæœ¬ä¸º4.49.0..."
pip install git+https://github.com/huggingface/transformers.git@1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf
pip install transformers==4.49.0

# å®‰è£…requirements.txtä¸­çš„å…¶ä»–ä¾èµ–
echo ""
echo "å®‰è£…requirements.txtä¸­çš„å…¶ä»–ä¾èµ–..."
pip install -r requirements.txt

# æ£€æŸ¥wandbç™»å½•çŠ¶æ€
echo ""
echo "æ£€æŸ¥wandbç™»å½•çŠ¶æ€..."
if ! wandb status 2>/dev/null | grep -q "Logged in"; then
    echo "âš ï¸  æ‚¨å°šæœªç™»å½•W&B (Weights & Biases)"
    echo "W&Bç”¨äºè®­ç»ƒè¿‡ç¨‹çš„å¯è§†åŒ–å’Œæ—¥å¿—è®°å½•"
    read -p "æ˜¯å¦ç°åœ¨ç™»å½•W&B? (y/n) " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        read -rs -p "è¯·è¾“å…¥æ‚¨çš„W&B API key: " WANDB_KEY
        echo
        wandb login "$WANDB_KEY"
    else
        echo "æ‚¨å¯ä»¥ç¨åä½¿ç”¨ 'wandb login' å‘½ä»¤ç™»å½•"
    fi
else
    echo "âœ“ W&Bå·²ç™»å½•"
fi

echo ""
echo "=========================================="
echo "å®‰è£…å®Œæˆï¼"
echo "=========================================="
echo ""
echo "ç¯å¢ƒä¿¡æ¯:"
echo "  - Condaç¯å¢ƒ: ${CONDA_DEFAULT_ENV:-æœªè®¾ç½®}"
echo "  - Pythonç‰ˆæœ¬: $PYTHON_VERSION"
echo "  - CUDAç‰ˆæœ¬: $CUDA_VERSION"
echo "  - PyTorch CUDA: $PYTORCH_CUDA"
echo ""
echo "ä¸‹ä¸€æ­¥:"
echo "  1. å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆå¦‚éœ€è¦ï¼‰:"
echo "     python scripts/convert_test_data.py --input_dir <æ•°æ®ç›®å½•> --output_jsonl <è¾“å‡ºæ–‡ä»¶>"
echo "  2. é…ç½®è®­ç»ƒå‚æ•°ï¼ˆç¼–è¾‘ train_examples/test_qwen3vl_8b_config.yamlï¼‰"
echo "  3. è¿è¡ŒRLè®­ç»ƒè„šæœ¬:"
echo "     bash ./train_examples/start_test_train.sh        # éäº¤äº’å¼ï¼ˆæ¨èï¼‰"
echo "     æˆ–"
echo "     bash ./train_examples/test_qwen3vl_8b_train.sh   # äº¤äº’å¼"
echo ""
echo "å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒ SETUP.md æˆ– README.md æ–‡æ¡£"
echo ""

